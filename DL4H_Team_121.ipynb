{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/just99kim/CS598/blob/main/DL4H_Team_121.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Link"
      ],
      "metadata": {
        "id": "cU_0vZfOU4iB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Link](https://drive.google.com/file/d/1cupg5eDzopz26ZIspRYMFvHcQ1ZHorpC/view?usp=sharing)"
      ],
      "metadata": {
        "id": "Il-RzQBUU8Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "*   Background\n",
        "  * Alzheimer’s Disease (AD) is the most common neurodegenerative disorder, characterized by complex pathogenesis that complicates early diagnosis. Accurate early diagnosis, particularly distinguishing MCI from AD, is crucial for delaying disease progression through timely intervention.\n",
        "*   Paper explanation\n",
        "  * This paper seeks to build upon the groundbreaking work presented in the development of the Multimodal Alzheimer’s Disease Diagnosis framework (MADDi), an attention-based deep learning model designed for the accurate diagnosis of Alzheimer's Disease (AD) through the integration of imaging, genetic, and clinical data [1]. The original study demonstrates the effectiveness of this model, achieving state-of-the-art performance with an accuracy of 96.88% in distinguishing between control, Mild Cognitive Impairment (MCI), and Alzheimer’s Disease."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "I will test the following hypothesis from the paper.\n",
        "\n",
        "1. MADDi classifies MCI, AD, and controls with significant accuracy on a held-out test set.\n",
        "2. MADDi classifies MCI, AD, and controls with higher accuracy when compared to unimodal models."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Source of the data: The data is collected from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The data was not provided in this repository and needs to be requested directly from the ADNI [here](https://adni.loni.usc.edu/data-samples/access-data/).\n",
        "  * Data Preprocessing: A list of data patient IDs with their diagnoses were created. Clinical, imaging and genetic data were preprocessed accordingly.\n",
        "  * Training & Valuation: A uni-modal model baseline (i.e. clinical, genetic and images) and multimodal architecture were trained and evaluated separately."
      ],
      "metadata": {
        "id": "ZaEufRibWddh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Environment"
      ],
      "metadata": {
        "id": "kzASSVPkh4Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python 3.7.4 (and above)\n",
        "Tensorflow 2.6.0 are required to run this notebook.\n",
        "\n",
        "Further details on all package requirements used in this repository can be found below\n",
        "\n"
      ],
      "metadata": {
        "id": "uQn6EqnGh51O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "absl-py==0.14.1\n",
        "aiohttp==3.8.1\n",
        "aiosignal==1.2.0\n",
        "astunparse==1.6.3\n",
        "async-timeout==4.0.1\n",
        "asynctest==0.13.0\n",
        "attrs==21.2.0\n",
        "cached-property==1.5.2\n",
        "cachetools==4.2.4\n",
        "certifi==2021.10.8\n",
        "charset-normalizer==2.0.6\n",
        "clang==5.0\n",
        "click==8.0.3\n",
        "cycler==0.10.0\n",
        "datasets==1.15.1\n",
        "dill==0.3.4\n",
        "filelock==3.4.0\n",
        "flatbuffers==1.12\n",
        "frozenlist==1.2.0\n",
        "fsspec==2021.11.0\n",
        "gast==0.4.0\n",
        "google-auth==1.35.0\n",
        "google-auth-oauthlib==0.4.6\n",
        "google-pasta==0.2.0\n",
        "grpcio==1.41.0\n",
        "h5py==3.1.0\n",
        "idna==3.2\n",
        "imageio==2.9.0\n",
        "importlib-metadata==4.8.1\n",
        "joblib==1.1.0\n",
        "keras==2.6.0\n",
        "Keras-Preprocessing==1.1.2\n",
        "kiwisolver==1.3.2\n",
        "Markdown==3.3.4\n",
        "matplotlib==3.4.3\n",
        "multidict==5.2.0\n",
        "multiprocess==0.70.12.2\n",
        "networkx==2.6.3\n",
        "nibabel==3.2.1\n",
        "numpy==1.19.5\n",
        "oauthlib==3.1.1\n",
        "opt-einsum==3.3.0\n",
        "packaging==21.0\n",
        "pandas==1.3.3\n",
        "pickle5==0.0.12\n",
        "Pillow==8.3.2\n",
        "protobuf==3.18.1\n",
        "pyarrow==6.0.1\n",
        "pyasn1==0.4.8\n",
        "pyasn1-modules==0.2.8\n",
        "pyparsing==2.4.7\n",
        "python-dateutil==2.8.2\n",
        "pytz==2021.3\n",
        "PyWavelets==1.1.1\n",
        "PyYAML==6.0\n",
        "regex==2021.11.10\n",
        "requests==2.26.0\n",
        "requests-oauthlib==1.3.0\n",
        "rsa==4.7.2\n",
        "sacremoses==0.0.46\n",
        "scikit-image==0.18.3\n",
        "scikit-learn==1.0\n",
        "scipy==1.7.1\n",
        "six==1.15.0\n",
        "tensorboard==2.6.0\n",
        "tensorboard-data-server==0.6.1\n",
        "tensorboard-plugin-wit==1.8.0\n",
        "tensorflow==2.6.0\n",
        "tensorflow-addons==0.14.0\n",
        "tensorflow-estimator==2.6.0\n",
        "termcolor==1.1.0\n",
        "threadpoolctl==3.0.0\n",
        "tifffile==2021.8.30\n",
        "tokenizers==0.10.3\n",
        "tqdm==4.62.3\n",
        "typeguard==2.12.1\n",
        "typing-extensions==3.7.4.3\n",
        "urllib3==1.26.7\n",
        "Werkzeug==2.0.2\n",
        "wrapt==1.12.1\n",
        "xxhash==2.0.2\n",
        "yarl==1.7.2\n",
        "zipp==3.6.0"
      ],
      "metadata": {
        "id": "ZsvmyGQMiUJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (clinical, genetic, images) and preprocessing.\n",
        "\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Data\n",
        "\n",
        "Data was downlaoded from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The data was not provided in this repository and needs to be requested directly from the ADNI here."
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a list of patient IDs with their diagnosis,**"
      ],
      "metadata": {
        "id": "tjfJFGTmQqkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method take diagnosis from images, clinical, and diagnosis sheet, and creates one ground truth (where all three agree) and one majority vote (where two agree) diagnosis files."
      ],
      "metadata": {
        "id": "zMr7gT7jRNBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "clinical = pd.read_csv(\"/content/drive/My Drive/ADSP_PHC_COGN.csv\").rename(columns={\"PHASE\":\"Phase\"})\n",
        "#this file is the metadata file that one can get from downloading MRI images from ADNI\n",
        "img = pd.read_csv(\"/content/drive/My Drive/MRILIST_07May2024.csv\")\n",
        "comb = pd.read_csv(\"/content/drive/My Drive/DXSUM_PDXCONV.csv\")[[\"RID\", \"PTID\" , \"PHASE\"]]"
      ],
      "metadata": {
        "id": "-M0FvO_eQwHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_diagnose(file_path: str = '/content/drive/My Drive/DXSUM_PDXCONV.csv', verbose=False):\n",
        "    # Read diagnostic summary\n",
        "    diagnostic_summary = pd.read_csv(file_path, index_col='PTID')\n",
        "    diagnostic_summary = diagnostic_summary.sort_values(by=[\"update_stamp\"], ascending=True)\n",
        "    # Create dictionary\n",
        "    diagnostic_dict: dict = {}\n",
        "    for key, data in diagnostic_summary.iterrows():\n",
        "        # Iterate for each row of the document\n",
        "        phase: str = data['PHASE']\n",
        "        diagnosis: float = -1.\n",
        "        if phase == \"ADNI1\":\n",
        "            diagnosis = data['DIAGNOSIS']\n",
        "        elif phase == \"ADNI2\" or phase == \"ADNIGO\":\n",
        "            dxchange = data['DIAGNOSIS']\n",
        "            if dxchange == 1 or dxchange == 7 or dxchange == 9:\n",
        "                diagnosis = 1.\n",
        "            if dxchange == 2 or dxchange == 4 or dxchange == 8:\n",
        "                diagnosis = 2.\n",
        "            if dxchange == 3 or dxchange == 5 or dxchange == 6:\n",
        "                diagnosis = 3.\n",
        "        elif phase == \"ADNI3\":\n",
        "            diagnosis = data['DIAGNOSIS']\n",
        "        else:\n",
        "            print(f\"ERROR: Not recognized study phase {phase}\")\n",
        "            exit(1)\n",
        "        # Update dictionary\n",
        "        if not math.isnan(diagnosis):\n",
        "            diagnostic_dict[key] = diagnosis\n",
        "    if verbose:\n",
        "        print_diagnostic_dict_summary(diagnostic_dict)\n",
        "    return diagnostic_dict\n",
        "\n",
        "\n",
        "def print_diagnostic_dict_summary(diagnostic_dict: dict):\n",
        "    print(f\"Number of diagnosed patients: {len(diagnostic_dict.items())}\\n\")\n",
        "    n_NL = 0\n",
        "    n_MCI = 0\n",
        "    n_AD = 0\n",
        "    for (key, data) in diagnostic_dict.items():\n",
        "        if data == 1:\n",
        "            n_NL += 1\n",
        "        if data == 2:\n",
        "            n_MCI += 1\n",
        "        if data == 3:\n",
        "            n_AD += 1\n",
        "    print(f\"Number of NL patients: {n_NL}\\n\"\n",
        "          f\"Number of MCI patients: {n_MCI}\\n\"\n",
        "          f\"Number of AD patients: {n_AD}\\n\")"
      ],
      "metadata": {
        "id": "d6_OJ1SDRPLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = read_diagnose()\n",
        "print_diagnostic_dict_summary(d)"
      ],
      "metadata": {
        "id": "9PHOEIv9RYfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.DataFrame.from_dict(d, orient='index').reset_index()"
      ],
      "metadata": {
        "id": "lFFcltLSRZtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinical.head()"
      ],
      "metadata": {
        "id": "7PMgjcbHRay2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinical[\"year\"] = clinical[\"EXAMDATE\"].str[:4]"
      ],
      "metadata": {
        "id": "HAQJ9ow8Rbxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinical[\"Subject\"] = clinical[\"SUBJECT_KEY\"].str.replace(\"ADNI_\", \"\").str.replace(\"s\", \"S\")"
      ],
      "metadata": {
        "id": "yVSYw_6DRfCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinical.rename(columns={'Phase': 'PHASE'}, inplace=True)\n",
        "comb.rename(columns={'phase': 'PHASE'}, inplace=True)\n",
        "\n",
        "c = comb.merge(clinical, on = [\"RID\", \"PHASE\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "X5hnUVitRgWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = c.drop(\"Subject\", axis =1)"
      ],
      "metadata": {
        "id": "bneb9xZHRhsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = c.rename(columns = {\"PTID\":\"Subject\"})"
      ],
      "metadata": {
        "id": "yAVC-zJiRjns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img[\"year\"] = img[\"SCANDATE\"].str[5:].str.replace(\"/\", \"\")"
      ],
      "metadata": {
        "id": "dGHdcP0eRkHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = img.replace([\"CN\", \"MCI\", \"AD\"], [ 0, 1, 2])"
      ],
      "metadata": {
        "id": "34dDkuAuRla4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c[\"DX\"] = c[\"DX\"] -1"
      ],
      "metadata": {
        "id": "xUVfnXVxRmjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new[0] = new[0].astype(int) -1"
      ],
      "metadata": {
        "id": "hNC_AZxhRn6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = new.rename(columns = {\"index\":\"Subject\", 0:\"GroupN\"})"
      ],
      "metadata": {
        "id": "w_C2utU8Ro_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new.rename(columns={'Subject': 'SUBJECT'}, inplace=True)\n",
        "img.rename(columns={'SUBJECT': 'SUBJECT'}, inplace=True)\n",
        "c.rename(columns={'Subject': 'SUBJECT'}, inplace=True)\n",
        "\n",
        "\n",
        "m = new.merge(c, on = 'SUBJECT', how = \"outer\").merge(img, on = 'SUBJECT', how = \"outer\")"
      ],
      "metadata": {
        "id": "aHvsXCW_RqDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(m.columns)\n",
        "\n",
        "m[[\"GroupN\", \"DX\", \"RID\"]]"
      ],
      "metadata": {
        "id": "N4-En3HTRrjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = m[[\"SUBJECT\", \"GroupN\", \"RID\", \"DX\", \"PHASE\"]].drop_duplicates()"
      ],
      "metadata": {
        "id": "r2zXsiB_Rsro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = m.dropna(subset = [\"GroupN\", \"RID\", \"DX\"], how=\"all\").drop_duplicates()\n",
        "m"
      ],
      "metadata": {
        "id": "5HAbb6VWRtv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.loc[m[\"DX\"].isna() & m[\"RID\"].isna(), \"RID\"] = m.loc[m[\"DX\"].isna() & m[\"RID\"].isna(), \"GroupN\"]\n",
        "m.loc[m[\"DX\"].isna() & m[\"RID\"].isna(), \"DX\"] = m.loc[m[\"DX\"].isna() & m[\"RID\"].isna(), \"GroupN\"]"
      ],
      "metadata": {
        "id": "uglGd5FcRvEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = m[m[\"GroupN\"] == m[\"RID\"]]\n",
        "m3 = m[m[\"GroupN\"] == m[\"DX\"]]\n",
        "m4 = m[m[\"RID\"] == m[\"DX\"]]\n",
        "m2 = m1[m1[\"RID\"] == m1[\"DX\"]]"
      ],
      "metadata": {
        "id": "qErvkN_CRw1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = m1[[\"SUBJECT\", \"GroupN\", \"RID\", \"DX\", \"PHASE\"]]\n",
        "m1"
      ],
      "metadata": {
        "id": "voOVqmJzRyvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1.loc[m1[\"DX\"].isna(), \"DX\"] = m1.loc[m1[\"DX\"].isna(), \"RID\"]"
      ],
      "metadata": {
        "id": "Ld8tlfvMR0CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m3 = m3[[\"SUBJECT\", \"GroupN\", \"RID\", \"DX\", \"PHASE\"]]\n",
        "m3"
      ],
      "metadata": {
        "id": "flSK7lMFR1OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m3.loc[m3[\"RID\"].isna(), \"RID\"] = m3.loc[m3[\"RID\"].isna(), \"GroupN\"]"
      ],
      "metadata": {
        "id": "cT1vyiDzR2bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m4 = m4[[\"SUBJECT\", \"GroupN\", \"RID\", \"DX\", \"PHASE\"]]\n",
        "m4"
      ],
      "metadata": {
        "id": "NSOpuvE4R3-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m4[m4[\"GroupN\"] != m4[\"DX\"]]"
      ],
      "metadata": {
        "id": "CggiDQgUR5Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m2[[\"SUBJECT\", \"GroupN\", \"RID\", \"DX\", \"PHASE\"]]"
      ],
      "metadata": {
        "id": "XVjgVqcKR6IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m5 = pd.concat([m1,m3,m4])\n",
        "i = m5[m5[\"RID\"] == m5[\"GroupN\"]]\n",
        "i = i[i[\"RID\"] == i[\"DX\"]]"
      ],
      "metadata": {
        "id": "hvHO3zY8R7fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = i.drop_duplicates()"
      ],
      "metadata": {
        "id": "hxUO7dUFR8us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i"
      ],
      "metadata": {
        "id": "TVtPV6JwR970"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i[[\"SUBJECT\", \"RID\", \"PHASE\"]].to_csv(\"ground_truth.csv\")"
      ],
      "metadata": {
        "id": "EV0z72ueSAI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.update(m5[~m5.index.duplicated(keep='first')])"
      ],
      "metadata": {
        "id": "qSFFB4NHSBWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = m.index"
      ],
      "metadata": {
        "id": "vtZTSouqSCp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if none of the three diagnosis agree, then we set the value to -1\n",
        "m[\"GROUP\"] = -1"
      ],
      "metadata": {
        "id": "yg5DfrQnSDrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in indexes:\n",
        "    row = m.loc[i]\n",
        "    if (row[\"GroupN\"] == row[\"RID\"]):\n",
        "        val = row[\"GroupN\"]\n",
        "\n",
        "        m.loc[i, \"GROUP\"] = val\n",
        "    elif (row[\"GroupN\"] == row[\"DX\"]):\n",
        "        val = row[\"GroupN\"]\n",
        "        m.loc[i, \"GROUP\"] = val\n",
        "\n",
        "    elif (row[\"RID\"] == row[\"DX\"]):\n",
        "        val = row[\"Group\"]\n",
        "        m.loc[i, \"GROUP\"] = val"
      ],
      "metadata": {
        "id": "OHg59YSbSE34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m5 = m5[~m5.index.duplicated(keep='first')]\n",
        "m5"
      ],
      "metadata": {
        "id": "V_-YwDIeSGr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m[m[\"GROUP\"] != -1]"
      ],
      "metadata": {
        "id": "1JXdKf-7SH4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m[[\"SUBJECT\", \"GroupN\", \"RID\", \"DX\", \"GROUP\", \"PHASE\"]].to_csv(\"diagnosis_full.csv\")"
      ],
      "metadata": {
        "id": "UWfGJ_RJSI7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Clinical Data**"
      ],
      "metadata": {
        "id": "zntJQPDQKnGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This series of methods will preprocess the clinical data and create a CSV file with the necessary data."
      ],
      "metadata": {
        "id": "a0FdLVe1SSPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import necesary packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,MaxPooling1D, Flatten,BatchNormalization, GaussianNoise,Conv1D\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import compute_class_weight\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this was created in the first step\n",
        "diag = pd.read_csv(\"ground_truth.csv\").drop(\"Unnamed: 0\", axis=1)"
      ],
      "metadata": {
        "id": "TTwWVUShKEBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we are combining several clinical datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "ro8P1Uz0KJX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo = pd.read_csv(\"PTDEMOG.csv\")"
      ],
      "metadata": {
        "id": "iFJpj-O2KHun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuro = pd.read_csv(\"NEUROEXM.csv\")\n",
        "neuro.columns"
      ],
      "metadata": {
        "id": "_8NPwhbSKNUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinical = pd.read_csv(\"ADSP_PHC_COGN.csv\").rename(columns={\"PHASE\":\"Phase\"})"
      ],
      "metadata": {
        "id": "O_73jRDbKOvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinical.head()"
      ],
      "metadata": {
        "id": "ymHit7EDPM1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diag[\"Subject\"].value_counts()\n"
      ],
      "metadata": {
        "id": "FfQHv1EqKU35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb = pd.read_csv(\"DXSUM_PDXCONV_ADNIALL.csv\")[[\"RID\", \"PTID\" , \"Phase\"]]\n"
      ],
      "metadata": {
        "id": "eoFVnZ3pO57k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = comb.merge(demo, on = [\"RID\", \"Phase\"]).merge(neuro,on = [\"RID\", \"Phase\"]).merge(clinical,on = [\"RID\", \"Phase\"]).drop_duplicates()"
      ],
      "metadata": {
        "id": "lwgmlERjO79M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.columns = [c[:-2] if str(c).endswith(('_x','_y')) else c for c in m.columns]\n",
        "m = m.loc[:,~m.columns.duplicated()]\n"
      ],
      "metadata": {
        "id": "fGTDCngwPQEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diag = diag.rename(columns = {\"Subject\": \"PTID\"})\n"
      ],
      "metadata": {
        "id": "ILjDdPwoPAGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = m.merge(diag, on = [\"PTID\", \"Phase\"])"
      ],
      "metadata": {
        "id": "2uCkxkY2PBVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m[\"PTID\"].value_counts()\n"
      ],
      "metadata": {
        "id": "vg7ITdI2PCzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = m\n"
      ],
      "metadata": {
        "id": "-FpcZOaMPD-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = t.drop([\"ID\",  \"SITEID\", \"VISCODE\", \"VISCODE2\", \"USERDATE\", \"USERDATE2\",\n",
        "            \"update_stamp\",  \"PTSOURCE\", \"PTDOBMM\",\"DX\"], axis=1)"
      ],
      "metadata": {
        "id": "EWMGBzztPb-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.columns"
      ],
      "metadata": {
        "id": "xprPjOdMPdsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = t.fillna(-4)\n",
        "t = t.replace(\"-4\", -4)\n",
        "cols_to_delete = t.columns[(t == -4).sum()/len(t) > .70]\n",
        "t.drop(cols_to_delete, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "71Z4iCF5Pg0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t.columns)"
      ],
      "metadata": {
        "id": "zCiow-h1PiQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t[\"PTWORK\"] = t[\"PTWORK\"].str.lower().str.replace(\"housewife\", \"homemaker\").str.replace(\"rn\", \"nurse\").str.replace(\"bookeeper\",\n"
      ],
      "metadata": {
        "id": "vgI4NWcJPkpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t[\"PTWORK\"] = t[\"PTWORK\"].fillna(\"-4\").astype(str)\n"
      ],
      "metadata": {
        "id": "Ne7-VUIIPl95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*teach.*$)', 'education')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*bookkeep.*$)', 'bookkeeper')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*wife.*$)', 'homemaker')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*educat.*$)', 'education')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*engineer.*$)', 'engineer')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*eingineering.*$)', 'engineer')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*computer programmer.*$)', 'engineer')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*nurs.*$)', 'nurse')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*manage.*$)', 'managment')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*therapist.*$)', 'therapist')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*sales.*$)', 'sales')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*admin.*$)', 'admin')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*account.*$)', 'accounting')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*real.*$)', 'real estate')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*secretary.*$)', 'secretary')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*professor.*$)', 'professor')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*chem.*$)', 'chemist')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*business.*$)', 'business')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*writ.*$)', 'writing')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*psych.*$)', 'psychology')\n",
        "t['PTWORK'] = t['PTWORK'].str.replace(r'(^.*analys.*$)', 'analyst')"
      ],
      "metadata": {
        "id": "PC5TR0HhPmcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cond = t['PTWORK'].value_counts()\n",
        "threshold = 10\n",
        "t['PTWORK'] = np.where(t['PTWORK'].isin(cond.index[cond >= threshold ]), t['PTWORK'], 'other')"
      ],
      "metadata": {
        "id": "R3m2cKtEPoLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = ['PTGENDER', 'PTWORK',\n",
        " 'PTHOME',\n",
        " 'PTMARRY',\n",
        " 'PTEDUCAT',\n",
        " 'PTPLANG',\n",
        " 'NXVISUAL',\n",
        " 'PTNOTRT',\n",
        " 'NXTREMOR',\n",
        " 'NXAUDITO',\n",
        " 'PTHAND']\n"
      ],
      "metadata": {
        "id": "HTJ5_0MWPp4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant = ['PTDOBYY',\n",
        " 'PHC_MEM',\n",
        " 'PHC_EXF',\n",
        " 'PTRACCAT',\n",
        " 'AGE',\n",
        " 'PTADDX',\n",
        " 'PTETHCAT',\n",
        " 'PTCOGBEG',\n",
        " 'PHC_VSP',\n",
        " 'PHC_LAN']\n"
      ],
      "metadata": {
        "id": "nTWrYPDRPrlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"PTWORK\", \"CMMED\"]\n"
      ],
      "metadata": {
        "id": "SwfcoDDTPtNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_left = list(set(t.columns) - set(categorical) - set(text)  - set([\"label\", \"Group\",\"GROUP\", \"Phase\", \"RID\", \"PTID\"]))\n",
        "t[cols_left]\n"
      ],
      "metadata": {
        "id": "ljO54G1ePuc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in cols_left:\n",
        "    if len(t[col].value_counts()) < 10:\n",
        "        print(col)\n",
        "        categorical.append(col)\n"
      ],
      "metadata": {
        "id": "Jb2otqZ2Pvo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_del = [\"PTRTYR\", \"EXAMDATE\", \"SUBJECT_KEY\", \"PTWRECNT\"]\n",
        "t = t.drop(to_del, axis=1)\n"
      ],
      "metadata": {
        "id": "wwviOMzwPw0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant = list(set(cols_left) - set(categorical) - set(text)  -set(to_del) - set([\"label\", \"Group\",\"GROUP\", \"Phase\", \"RID\", \"PTID\"]))\n",
        "t[quant]\n"
      ],
      "metadata": {
        "id": "QdlRxTZqPx-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_left = list(set(cols_left) - set(categorical) - set(text) - set(quant) - set(to_del))\n"
      ],
      "metadata": {
        "id": "AVl2ibkAPzN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after reviewing the meaning of each column, these are the final ones\n",
        "l = ['RID', 'PTID', 'Group', 'Phase', 'PTGENDER', 'PTDOBYY', 'PTHAND',\n",
        "       'PTMARRY', 'PTEDUCAT', 'PTWORK', 'PTNOTRT', 'PTHOME', 'PTTLANG',\n",
        "       'PTPLANG', 'PTCOGBEG', 'PTETHCAT', 'PTRACCAT', 'NXVISUAL',\n",
        "       'NXAUDITO', 'NXTREMOR', 'NXCONSCI', 'NXNERVE', 'NXMOTOR', 'NXFINGER',\n",
        "       'NXHEEL', 'NXSENSOR', 'NXTENDON', 'NXPLANTA', 'NXGAIT',\n",
        "       'NXABNORM',  'PHC_MEM', 'PHC_EXF', 'PHC_LAN', 'PHC_VSP']\n"
      ],
      "metadata": {
        "id": "qNGO6Pq0P0Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t[l]"
      ],
      "metadata": {
        "id": "mCAu-69jP14_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []"
      ],
      "metadata": {
        "id": "cZjGQYkRP29N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical:\n",
        "    dfs.append(pd.get_dummies(t[col], prefix = col))"
      ],
      "metadata": {
        "id": "8d4IWEs4P4Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat = pd.concat(dfs, axis=1)"
      ],
      "metadata": {
        "id": "JuvZn9j0P57h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t[quant]"
      ],
      "metadata": {
        "id": "A01oJsNbP8KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat"
      ],
      "metadata": {
        "id": "_UGZ8s-5P9Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t[[\"PTID\",\"RID\", \"Phase\", \"Group\"]]"
      ],
      "metadata": {
        "id": "_Zjex9mcP-lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = pd.concat([t[[\"PTID\", \"RID\", \"Phase\", \"Group\"]].reset_index(), cat.reset_index(), t[quant].reset_index()], axis=1).drop(\"index\", axis=1) #tex"
      ],
      "metadata": {
        "id": "s3q0Ew9PQAFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "id": "hgg9EAi9QBfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing repeating subjects, taking the most recent diagnosis\n",
        "c = c.groupby('PTID',\n",
        "                  group_keys=False).apply(lambda x: x.loc[x[\"Group\"].astype(int).idxmax()]).drop(\"PTID\", axis = 1).reset_index(inplace=False)\n"
      ],
      "metadata": {
        "id": "LCBuMtRnQDSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.to_csv(\"clinical.csv\")"
      ],
      "metadata": {
        "id": "59zOHWyFQFxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading in the overlap test set\n",
        "ts = pd.read_csv(\"overlap_test_set.csv\").rename(columns={\"subject\": \"PTID\"})\n",
        "\n",
        "#removing ids from the overlap test set\n",
        "c = c[~c[\"PTID\"].isin(list(ts[\"PTID\"].values))]\n"
      ],
      "metadata": {
        "id": "3TXU7LNAQG_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = list(set(c.columns) - set([\"PTID\",\"RID\",\"subject\", \"ID\",\"GROUP\", \"Group\", \"label\", \"Phase\", \"SITEID\", \"VISCODE\", \"VISCODE2\", \"USERDATE\", \"USERDATE2\", \"update_stamp\", \"DX_x\",\"DX_y\", \"Unnamed: 0\"]))\n",
        "X = c[cols].values\n",
        "y = c[\"Group\"].astype(int).values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "ATDRVwyDQIY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_pickle(\"X_train_c.pkl\")\n",
        "y_train.to_pickle(\"y_train_c.pkl\")\n",
        "\n",
        "X_test.to_pickle(\"X_test_c.pkl\")\n",
        "y_test.to_pickle(\"y_test_c.pkl\")"
      ],
      "metadata": {
        "id": "o6IeVk4LQJ9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  return None\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  return None\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "  return None\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Images Data**"
      ],
      "metadata": {
        "id": "Ky7EG_CpScEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following python script with the directory where images are stored as the argument."
      ],
      "metadata": {
        "id": "JyNa1EDMSlV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import skimage.transform as skTrans\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "\n",
        "def normalize_img(img_array):\n",
        "    maxes = np.quantile(img_array,0.995,axis=(0,1,2))\n",
        "    #print(\"Max value for each modality\", maxes)\n",
        "    return img_array/maxes\n",
        "\n",
        "\n",
        "def create_dataset(meta, meta_all,path_to_datadir):\n",
        "    files = os.listdir(path_to_datadir)\n",
        "    start = '_'\n",
        "    end = '.nii'\n",
        "    for file in files:\n",
        "        print(file)\n",
        "        if file != '.DS_Store':\n",
        "            path = os.path.join(path_to_datadir, file)\n",
        "            print(path)\n",
        "            img_id = file.split(start)[-1].split(end)[0]\n",
        "            idx = meta[meta[\"Image Data ID\"] == img_id].index[0]\n",
        "            im = nib.load(path).get_fdata()\n",
        "            n_i, n_j, n_k = im.shape\n",
        "            center_i = (n_i - 1) // 2\n",
        "            center_j = (n_j - 1) // 2\n",
        "            center_k = (n_k - 1) // 2\n",
        "            im1 = skTrans.resize(im[center_i, :, :], (72, 72), order=1, preserve_range=True)\n",
        "            im2 = skTrans.resize(im[:, center_j, :], (72, 72), order=1, preserve_range=True)\n",
        "            im3 = skTrans.resize(im[:, :, center_k], (72, 72), order=1, preserve_range=True)\n",
        "            im = np.array([im1,im2,im3]).T\n",
        "            label = meta.at[idx, \"Group\"]\n",
        "            subject = meta.at[idx, \"Subject\"]\n",
        "            norm_im = normalize_img(im)\n",
        "            meta_all = meta_all.append({\"img_array\": im,\"label\": label,\"subject\":subject}, ignore_index=True)\n",
        "\n",
        "\n",
        "    meta_all.to_pickle(\"mri_meta.pkl\")\n",
        "    meta_all.flush()\n",
        "    os.fsync(meta_all.fileno())\n",
        "    time.sleep(0.5)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = sys.argv[1:]\n",
        "    path_to_meta = args[0]\n",
        "    path_to_datadir = args[1]\n",
        "    print(path_to_meta)\n",
        "\n",
        "\n",
        "    meta = pd.read_csv(path_to_meta)\n",
        "    print(\"opened meta\")\n",
        "    print(len(meta))\n",
        "    #get rid of not needed columns\n",
        "    meta = meta[[\"Image Data ID\", \"Group\", \"Subject\"]] #MCI = 0, CN =1, AD = 2\n",
        "    meta[\"Group\"] = pd.factorize(meta[\"Group\"])[0]\n",
        "    #initialize new dataset where arrays will go\n",
        "    meta_all = pd.DataFrame(columns = [\"img_array\",\"label\",\"subject\"])\n",
        "    create_dataset(meta, meta_all, path_to_datadir)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "mn3q_rmjSufr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the file created from the script to and use the following notebook to the imaging data into training and testing such that there are no repeating patients in the test set and that the patients in the test set do not appear in training.\n",
        "\n"
      ],
      "metadata": {
        "id": "r4-uLk7VTAJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "#reading in a dataframe that contains image arrays, patient IDs (\"subject\"), and diagnosis\n",
        "m2 = pd.read_pickle(\"mri_meta.pkl\")\n",
        "\n",
        "#cleaning patient IDs\n",
        "m2[\"subject\"] = m2[\"subject\"].str.replace(\"s\", \"S\").str.replace(\"\\n\", \"\")\n",
        "\n",
        "#reading in the overlap test set\n",
        "ts = pd.read_csv(\"overlap_test_set.csv\")\n",
        "\n",
        "#removing ids from the overlap test set\n",
        "m2 = m2[~m2[\"subject\"].isin(list(ts[\"subject\"].values))]"
      ],
      "metadata": {
        "id": "AHY_3gFoTGDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there are 551 unique patients\n",
        "subjects = list(set(m2[\"subject\"].values))\n",
        "len(subjects)"
      ],
      "metadata": {
        "id": "VSlqofw9TO2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0.1*len(m2) #10% for testing"
      ],
      "metadata": {
        "id": "XLZb1txdTQib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 3674 MRI scans from 551 patients (some patients repeated up to 16 times). We selected our testing set such that it has 367 unique MRIs (10% of training) shwon below. We do not allow for any repeating patients in the testing set. We only allowed repetition during training, and no patient was included in both training and testing sets."
      ],
      "metadata": {
        "id": "ypsR0VB5TTZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#selecting 367 patient IDs\n",
        "picked_ids = random.sample(subjects, 367)"
      ],
      "metadata": {
        "id": "TP1idjsLTRvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the test set out of the patient IDs\n",
        "test = pd.DataFrame(columns = [\"img_array\", \"subject\", \"label\"])\n",
        "for i in range(len(picked_ids)):\n",
        "    s = m2[m2[\"subject\"] == picked_ids[i]].sample()\n",
        "    test = test.append(s)"
      ],
      "metadata": {
        "id": "fwQ57bjrTW9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = list(set(m2.index) - set(test.index))"
      ],
      "metadata": {
        "id": "SKZPgh30TYFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the training set using all the other data points\n",
        "train = m2[m2.index.isin(indexes)]"
      ],
      "metadata": {
        "id": "syqrya8pTZVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[[\"img_array\"]].to_pickle(\"img_train.pkl\")\n",
        "test[[\"img_array\"]].to_pickle(\"img_test.pkl\")"
      ],
      "metadata": {
        "id": "b155OwoKTau9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[[\"label\"]].to_pickle(\"img_y_train.pkl\")\n",
        "test[[\"label\"]].to_pickle(\"img_y_test.pkl\")"
      ],
      "metadata": {
        "id": "ueX8jW6XTb4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Genetic Data**"
      ],
      "metadata": {
        "id": "bJThJevnTilT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First obtain VCF files from ADNI and then use the vcftools package to filter the files based on your chosen criteria (Hardy-Weinberg equilibrium, genotype quality, minor allele frequency, etc.)."
      ],
      "metadata": {
        "id": "CrDoPk0dTvBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Alzheimer's gene list"
      ],
      "metadata": {
        "id": "Rmqe6gCxVMak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/My Drive/gene_list.csv'\n"
      ],
      "metadata": {
        "id": "0cFcbaRqVLva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further filter VCF files according to AD-relrated genges from ALzGene Database (http://www.alzgene.org/)"
      ],
      "metadata": {
        "id": "Hs9PjwSpVgOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "\n",
        "def get_vcf_names(vcf_path):\n",
        "    with gzip.open(vcf_path, \"/content/drive/My Drive/002_snps\") as ifile:\n",
        "          for line in ifile:\n",
        "            if line.startswith(\"#CHROM\"):\n",
        "                vcf_names = [x for x in line.split('\\t')]\n",
        "                break\n",
        "    ifile.close()\n",
        "    return vcf_names\n",
        "\n",
        "\n",
        "def read_vcf(path):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = [l for l in f if not l.startswith('##')]\n",
        "    return pd.read_csv(\n",
        "        io.StringIO(''.join(lines)),\n",
        "        dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n",
        "               'QUAL': str, 'FILTER': str, 'INFO': str},\n",
        "        sep='\\t'\n",
        "    ).rename(columns={'#CHROM': 'CHROM'})\n",
        "\n",
        "def in_between(position, relevent):\n",
        "    appears = False\n",
        "    for i in range(len(relevent)):\n",
        "        row = relevent.iloc[i]\n",
        "        if (position >= relevent.iloc[i].start) and (position <= relevent.iloc[i].end):\n",
        "            appears = True\n",
        "    return appears\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    genes = pd.read_csv('/content/drive/My Drive/gene_list.csv')\n",
        "    files = os.listdir('/content/drive/My Drive/002_snps')\n",
        "\n",
        "\n",
        "    for vcf_file in files:\n",
        "        file_name = \"YOUR_PATH_TO_VCFS\" + vcf_file\n",
        "\n",
        "        output_file = open('log.txt','a')\n",
        "        output_file.write(file_name)\n",
        "        output_file.close()\n",
        "        names = get_vcf_names(file_name)\n",
        "        vcf = pd.read_csv(file_name, compression='gzip', comment='#', chunksize=10000, delim_whitespace=True, header=None, names=names)\n",
        "        vcf = pd.concat(vcf, ignore_index=True)\n",
        "\n",
        "        start = vcf_file.find(\"ADNI_ID.\") + len(\"ADNI_ID.\")\n",
        "        end = vcf_file.find(\"output.vcf\")\n",
        "        substring = vcf_file[start:end]\n",
        "        relevent = genes[genes[\"chrom\"] == substring]\n",
        "        relevent = relevent.reset_index()\n",
        "\n",
        "        positions = vcf[\"POS\"]\n",
        "\n",
        "\n",
        "        indexes = []\n",
        "        for i in range(len(positions)):\n",
        "\n",
        "            boo = in_between(positions[i], relevent)\n",
        "            if i % 500 == 0:\n",
        "                output_file = open('log.txt','a')\n",
        "                output_file.write(\" \" + str(boo) + \" \")\n",
        "                output_file.close()\n",
        "            if boo:\n",
        "                indexes.append(i)\n",
        "\n",
        "        if len(indexes) != 0:\n",
        "            df = vcf.iloc[indexes]\n",
        "            df.to_pickle(vcf_file[:-4] + \".pkl\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xi1Rke2WTx9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile all the genetic files together"
      ],
      "metadata": {
        "id": "UQP0xCB_VvLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "\n",
        "    files = os.listdir(\"YOUR_PATH_TO_FILTERED_VCFS\")\n",
        "    diag = pd.read_csv(\"YOUR_PATH_TO_DIAGNOSIS_TABLE\")[[\"index\", \"Group\"]]\n",
        "\n",
        "    vcfs = []\n",
        "\n",
        "    for vcf_file in files:\n",
        "        file_name = \"YOUR_PATH_TO_FILTERED_VCFS\" + vcf_file\n",
        "\n",
        "        vcf = pd.read_pickle(file_name)\n",
        "\n",
        "        vcf = vcf.drop(['#CHROM', 'POS', 'ID','REF','ALT','QUAL','FILTER','INFO', 'FORMAT'], axis=1)\n",
        "        vcf = vcf.T\n",
        "        vcf.reset_index(level=0, inplace=True)\n",
        "        vcf[\"index\"] = vcf[\"index\"].str.replace(\"s\", \"S\").str.replace(\"\\n\", \"\")\n",
        "        merged = diag.merge(vcf, on = \"index\")\n",
        "        merged = merged.rename(columns={\"index\": \"subject\"})\n",
        "        d = {'0/0': 0, '0/1': 1, '1/0': 1,  '1/1': 2, \"./.\": 3}\n",
        "        cols = list(set(merged.columns) - set([\"subject\", \"Group\"]))\n",
        "        for col in cols:\n",
        "            merged[col] = merged[col].str[:3].replace(d)\n",
        "            idx = cols.index(col)\n",
        "            if idx % 500 == 0:\n",
        "                output_file = open('log_clean.txt','a')\n",
        "                output_file.write(\"Percent done: \" + str((idx/len(cols))*100) + \"\\n\")\n",
        "                output_file.close()\n",
        "\n",
        "        merged.to_pickle(vcf_file + \"clean.pkl\")\n",
        "\n",
        "        vcf = vcf.groupby('index', group_keys=False).apply(lambda x: x.loc[x.Group.idxmax()])\n",
        "\n",
        "        vcfs.append(vcf)\n",
        "\n",
        "    vcf = pd.concat(vcfs, ignore_index=True)\n",
        "    vcf = vcf.drop_duplicates()\n",
        "    vcf.to_pickle(\"all_vcfs.pkl\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qgDKjiJQVyDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further reduce the number of features through with a Random Forest algorithm."
      ],
      "metadata": {
        "id": "LeBGS7WAV1ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reading all the SNP files\n",
        "vcf = pd.read_pickle(\"all_vcfs.pkl\")"
      ],
      "metadata": {
        "id": "HtGiU6DTWDvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading in the diagnosis data\n",
        "m = pd.read_csv(\"diagnosis_full.csv\").drop(\"Unnamed: 0\", axis=1).rename(columns = {\"Subject\":\"subject\", \"GROUP\": \"label\"})"
      ],
      "metadata": {
        "id": "YvXZZXvgWTTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making sure all the diagnosis agree\n",
        "m = m[m[\"label\"] != -1]"
      ],
      "metadata": {
        "id": "qEAKFurmWVFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging SNPs with diagnosis\n",
        "vcf = vcf.merge(m[[\"subject\", \"label\"]], on = \"subject\")"
      ],
      "metadata": {
        "id": "XeVHLrdGWWbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vcf = vcf.drop_duplicates()"
      ],
      "metadata": {
        "id": "DD1dNs9KWX89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading in the overlap test set\n",
        "ts = pd.read_csv(\"overlap_test_set.csv\")\n",
        "\n",
        "#removing ids from the overlap test set, saving it as a new variable\n",
        "vcf1 = vcf[~vcf[\"subject\"].isin(list(ts[\"subject\"].values))]"
      ],
      "metadata": {
        "id": "8GV26PvRWZTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Random Forest to reduce feature dimensions\n",
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))"
      ],
      "metadata": {
        "id": "ksGRUET_Waf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = list(set(vcf1.columns) - set([\"subject\", \"Group\", \"label\"]))\n",
        "X = vcf1[cols].values.astype(int)\n",
        "y = vcf1[\"label\"].astype(int).values\n",
        "\n",
        "for i in range(len(y)):\n",
        "    y[i] = y[i]-1"
      ],
      "metadata": {
        "id": "V_7zQL-uWbwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n"
      ],
      "metadata": {
        "id": "DqMW8qIHWdMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting random forest only on the training data so that there is not influence on the testing performance\n",
        "sel.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3nvXRIGYWeQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_feat= X_train.columns[(sel.get_support())]\n",
        "len(selected_feat)"
      ],
      "metadata": {
        "id": "pKB4dm_xWfY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_feat)"
      ],
      "metadata": {
        "id": "m7DEOxAjWgYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\"label\", \"subject\", \"Group\"]\n",
        "l.extend(selected_feat)"
      ],
      "metadata": {
        "id": "7Jo_IpvsWh0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the features with the old dataframe so that the overlap test set can still be used when combining all data\n",
        "vcf[l].to_pickle(\"vcf_select.pkl\")\n"
      ],
      "metadata": {
        "id": "pbiNJ7jqWi73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_pickle(\"X_train_vcf.pkl\")\n",
        "y_train.to_pickle(\"y_train_vcf.pkl\")\n",
        "\n",
        "X_test.to_pickle(\"X_test_vcf.pkl\")\n",
        "y_test.to_pickle(\"y_test_vcf.pkl\")"
      ],
      "metadata": {
        "id": "cnd3JJjXWkom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model/Training/Evaluation\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Citation"
      ],
      "metadata": {
        "id": "m0226RTli7Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Golovanevsky, Michal, Carsten Eickhoff, and Ritambhara Singh. \"Multimodal attention-based deep learning for Alzheimer’s disease diagnosis.\" Journal of the American Medical Informatics Association 29, no. 12 (2022): 2014–2022."
      ],
      "metadata": {
        "id": "T-cJcUmEi9Hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and evaluate a uni-modal model baseline for processed clinical data**"
      ],
      "metadata": {
        "id": "V_68S9stWxgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "\n",
        "def reset_random_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def main():\n",
        "    #this is created in the clinical preprocess jupyter notebook\n",
        "    X_train = pd.read_pickle(\"X_train_c.pkl\")\n",
        "    y_train = pd.read_pickle(\"y_train_c.pkl\")\n",
        "\n",
        "    X_test = pd.read_pickle(\"X_test_c.pkl\")\n",
        "    y_test = pd.read_pickle(\"y_test_c.pkl\")\n",
        "\n",
        "    acc = []\n",
        "    f1 = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    seeds = random.sample(range(1, 200), 5)\n",
        "    for seed in seeds:\n",
        "        reset_random_seeds(seed)\n",
        "        model = Sequential()\n",
        "        model.add(Dense(128, input_shape = (185,), activation = \"relu\"))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation = \"relu\"))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Dense(50, activation = \"relu\"))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "        model.compile(Adam(learning_rate = 0.0001), \"sparse_categorical_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "\n",
        "        history = model.fit(X_train, y_train,  epochs=100, validation_split=0.1, batch_size=32,verbose=1)\n",
        "\n",
        "        score = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "        acc.append(score[1])\n",
        "\n",
        "        test_predictions = model.predict(X_test)\n",
        "        test_label = to_categorical(y_test,3)\n",
        "\n",
        "        true_label= np.argmax(test_label, axis =1)\n",
        "\n",
        "        predicted_label= np.argmax(test_predictions, axis =1)\n",
        "\n",
        "        cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "        precision.append(cr[\"macro avg\"][\"precision\"])\n",
        "        recall.append(cr[\"macro avg\"][\"recall\"])\n",
        "        f1.append(cr[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "    print(\"Avg accuracy: \" + str(np.array(acc).mean()))\n",
        "    print(\"Avg precision: \" + str(np.array(precision).mean()))\n",
        "    print(\"Avg recall: \" + str(np.array(recall).mean()))\n",
        "    print(\"Avg f1: \" + str(np.array(f1).mean()))\n",
        "    print(\"Std accuracy: \" + str(np.array(acc).std()))\n",
        "    print(\"Std precision: \" + str(np.array(precision).std()))\n",
        "    print(\"Std recall: \" + str(np.array(recall).std()))\n",
        "    print(\"Std f1: \" + str(np.array(f1).std()))\n",
        "    print(acc)\n",
        "    print(precision)\n",
        "    print(recall)\n",
        "    print(f1)\n",
        "\n",
        "\n",
        "\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    #plt.savefig('snp_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "pyFsD2k9W5G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and evaluate a uni-modal model baseline for processed imaging data**"
      ],
      "metadata": {
        "id": "O3TJTxpSW81G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import pickle5 as pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers import Dense,Dropout,MaxPooling2D, Flatten, Conv2D\n",
        "\n",
        "\n",
        "def reset_random_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    with open(\"img_train.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    X_train_ = pd.DataFrame(data)[\"img_array\"]\n",
        "\n",
        "    with open(\"img_test.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    X_test_ = pd.DataFrame(data)[\"img_array\"]\n",
        "\n",
        "    with open(\"img_y_train.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    y_train = np.array(pd.DataFrame(data)[\"label\"].values.astype(np.float32)).flatten()\n",
        "\n",
        "    with open(\"img_y_test.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    y_test = np.array(pd.DataFrame(data)[\"label\"].values.astype(np.float32)).flatten()\n",
        "\n",
        "\n",
        "    y_test[y_test == 2] = -1\n",
        "    y_test[y_test == 1] = 2\n",
        "    y_test[y_test == -1] = 1\n",
        "\n",
        "    y_train[y_train == 2] = -1\n",
        "    y_train[y_train == 1] = 2\n",
        "    y_train[y_train == -1] = 1\n",
        "\n",
        "\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "\n",
        "    for i in range(len(X_train_)):\n",
        "        X_train.append(X_train_.values[i])\n",
        "\n",
        "    for i in range(len(X_test_)):\n",
        "        X_test.append(X_test_.values[i])\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "\n",
        "    acc = []\n",
        "    f1 = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    seeds = random.sample(range(1, 200), 5)\n",
        "    for seed in seeds:\n",
        "        reset_random_seeds(seed)\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(100, (3, 3),  activation='relu', input_shape=(72, 72, 3)))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Conv2D(50, (3, 3), activation='relu'))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "\n",
        "        model.compile(Adam(learning_rate = 0.001), \"sparse_categorical_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "\n",
        "        history = model.fit(X_train, y_train, epochs=50, batch_size=32,validation_split=0.1, verbose=1)\n",
        "\n",
        "        score = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "        acc.append(score[1])\n",
        "\n",
        "        test_predictions = model.predict(X_test)\n",
        "        test_label = to_categorical(y_test,3)\n",
        "\n",
        "        true_label= np.argmax(test_label, axis =1)\n",
        "\n",
        "        predicted_label= np.argmax(test_predictions, axis =1)\n",
        "\n",
        "        cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "        precision.append(cr[\"macro avg\"][\"precision\"])\n",
        "        recall.append(cr[\"macro avg\"][\"recall\"])\n",
        "        f1.append(cr[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "    print(\"Avg accuracy: \" + str(np.array(acc).mean()))\n",
        "    print(\"Avg precision: \" + str(np.array(precision).mean()))\n",
        "    print(\"Avg recall: \" + str(np.array(recall).mean()))\n",
        "    print(\"Avg f1: \" + str(np.array(f1).mean()))\n",
        "    print(\"Std accuracy: \" + str(np.array(acc).std()))\n",
        "    print(\"Std precision: \" + str(np.array(precision).std()))\n",
        "    print(\"Std recall: \" + str(np.array(recall).std()))\n",
        "    print(\"Std f1: \" + str(np.array(f1).std()))\n",
        "    print(acc)\n",
        "    print(precision)\n",
        "    print(recall)\n",
        "    print(f1)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "yoi-d1OvXKbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and evaluate a uni-modal model baseline for processed genetic data**"
      ],
      "metadata": {
        "id": "cOOc3rrmXLqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout, BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def reset_random_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "        #this is created in the genetic preprocess jupyter notebook\n",
        "        X_train = pd.read_pickle(\"X_train_vcf.pkl\")\n",
        "        y_train = pd.read_pickle(\"y_train_vcf.pkl\")\n",
        "\n",
        "        X_test = pd.read_pickle(\"X_test_vcf.pkl\")\n",
        "        y_test = pd.read_pickle(\"y_test_vcf.pkl\")\n",
        "\n",
        "\n",
        "        acc = []\n",
        "        f1 = []\n",
        "        precision = []\n",
        "        recall = []\n",
        "        seeds = random.sample(range(1, 200), 5)\n",
        "\n",
        "        for seed in seeds:\n",
        "            reset_random_seeds(seed)\n",
        "            model = Sequential()\n",
        "            model.add(Dense(128, input_shape = (15965,), activation = \"relu\"))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Dense(64, activation = \"relu\"))\n",
        "            model.add(Dropout(0.5))\n",
        "\n",
        "            model.add(Dense(32, activation = \"relu\"))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "            model.add(Dense(32, activation = \"relu\"))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "            model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "            model.compile(Adam(learning_rate = 0.001), \"sparse_categorical_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
        "\n",
        "\n",
        "            history = model.fit(X_train, y_train,epochs=50,batch_size=32,validation_split = 0.1, verbose=1)\n",
        "\n",
        "            score = model.evaluate(X_test, y_test, verbose=0)\n",
        "            print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "            acc.append(score[1])\n",
        "\n",
        "            test_predictions = model.predict(X_test)\n",
        "            test_label = to_categorical(y_test,3)\n",
        "\n",
        "            true_label= np.argmax(test_label, axis =1)\n",
        "\n",
        "            predicted_label= np.argmax(test_predictions, axis =1)\n",
        "\n",
        "            cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "            precision.append(cr[\"macro avg\"][\"precision\"])\n",
        "            recall.append(cr[\"macro avg\"][\"recall\"])\n",
        "            f1.append(cr[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "        print(\"Avg accuracy: \" + str(np.array(acc).mean()))\n",
        "        print(\"Avg precision: \" + str(np.array(precision).mean()))\n",
        "        print(\"Avg recall: \" + str(np.array(recall).mean()))\n",
        "        print(\"Avg f1: \" + str(np.array(f1).mean()))\n",
        "        print(\"Std accuracy: \" + str(np.array(acc).std()))\n",
        "        print(\"Std precision: \" + str(np.array(precision).std()))\n",
        "        print(\"Std recall: \" + str(np.array(recall).std()))\n",
        "        print(\"Std f1: \" + str(np.array(f1).std()))\n",
        "        print(acc)\n",
        "        print(precision)\n",
        "        print(recall)\n",
        "        print(f1)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ah4joXoHXPSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and evaluate the multimodal architecture**"
      ],
      "metadata": {
        "id": "cRjUO3IkXcYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import gc, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.utils import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, Dropout,Flatten, BatchNormalization, Conv2D, MultiHeadAttention, concatenate\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "def make_img(t_img):\n",
        "    img = pd.read_pickle(t_img)\n",
        "    img_l = []\n",
        "    for i in range(len(img)):\n",
        "        img_l.append(img.values[i][0])\n",
        "\n",
        "    return np.array(img_l)\n",
        "\n",
        "\n",
        "def reset_random_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def create_model_snp():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200,  activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(50, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    return model\n",
        "\n",
        "def create_model_clinical():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200,  activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(50, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    return model\n",
        "\n",
        "def create_model_img():\n",
        "\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(72, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def plot_classification_report(y_tru, y_prd, mode, learning_rate, batch_size,epochs, figsize=(7, 7), ax=None):\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
        "    yticks = [\"Control\", \"Moderate\", \"Alzheimer's\" ]\n",
        "    yticks += ['avg']\n",
        "\n",
        "    rep = np.array(precision_recall_fscore_support(y_tru, y_prd)).T\n",
        "    avg = np.mean(rep, axis=0)\n",
        "    avg[-1] = np.sum(rep[:, -1])\n",
        "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
        "\n",
        "    sns.heatmap(rep,\n",
        "                annot=True,\n",
        "                cbar=False,\n",
        "                xticklabels=xticks,\n",
        "                yticklabels=yticks,\n",
        "                ax=ax, cmap = \"Blues\")\n",
        "\n",
        "    plt.savefig('report_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'_' + str(epochs)+'.png')\n",
        "\n",
        "\n",
        "\n",
        "def calc_confusion_matrix(result, test_label,mode, learning_rate, batch_size, epochs):\n",
        "    test_label = to_categorical(test_label,3)\n",
        "\n",
        "    true_label= np.argmax(test_label, axis =1)\n",
        "\n",
        "    predicted_label= np.argmax(result, axis =1)\n",
        "\n",
        "    n_classes = 3\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    thres = dict()\n",
        "    for i in range(n_classes):\n",
        "        precision[i], recall[i], thres[i] = precision_recall_curve(test_label[:, i],\n",
        "                                                            result[:, i])\n",
        "\n",
        "\n",
        "    print (\"Classification Report :\")\n",
        "    print (classification_report(true_label, predicted_label))\n",
        "    cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "    return cr, precision, recall, thres\n",
        "\n",
        "\n",
        "\n",
        "def cross_modal_attention(x, y):\n",
        "    x = tf.expand_dims(x, axis=1)\n",
        "    y = tf.expand_dims(y, axis=1)\n",
        "    a1 = MultiHeadAttention(num_heads = 4,key_dim=50)(x, y)\n",
        "    a2 = MultiHeadAttention(num_heads = 4,key_dim=50)(y, x)\n",
        "    a1 = a1[:,0,:]\n",
        "    a2 = a2[:,0,:]\n",
        "    return concatenate([a1, a2])\n",
        "\n",
        "\n",
        "def self_attention(x):\n",
        "    x = tf.expand_dims(x, axis=1)\n",
        "    attention = MultiHeadAttention(num_heads = 4, key_dim=50)(x, x)\n",
        "    attention = attention[:,0,:]\n",
        "    return attention\n",
        "\n",
        "\n",
        "def multi_modal_model(mode, train_clinical, train_snp, train_img):\n",
        "\n",
        "    in_clinical = Input(shape=(train_clinical.shape[1]))\n",
        "\n",
        "    in_snp = Input(shape=(train_snp.shape[1]))\n",
        "\n",
        "    in_img = Input(shape=(train_img.shape[1], train_img.shape[2], train_img.shape[3]))\n",
        "\n",
        "    dense_clinical = create_model_clinical()(in_clinical)\n",
        "    dense_snp = create_model_snp()(in_snp)\n",
        "    dense_img = create_model_img()(in_img)\n",
        "\n",
        "\n",
        "\n",
        "    ########### Attention Layer ############\n",
        "\n",
        "    ## Cross Modal Bi-directional Attention ##\n",
        "\n",
        "    if mode == 'MM_BA':\n",
        "\n",
        "        vt_att = cross_modal_attention(dense_img, dense_clinical)\n",
        "        av_att = cross_modal_attention(dense_snp, dense_img)\n",
        "        ta_att = cross_modal_attention(dense_clinical, dense_snp)\n",
        "\n",
        "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ## Self Attention ##\n",
        "    elif mode == 'MM_SA':\n",
        "\n",
        "        vv_att = self_attention(dense_img)\n",
        "        tt_att = self_attention(dense_clinical)\n",
        "        aa_att = self_attention(dense_snp)\n",
        "\n",
        "        merged = concatenate([aa_att, vv_att, tt_att, dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "    ## Self Attention and Cross Modal Bi-directional Attention##\n",
        "    elif mode == 'MM_SA_BA':\n",
        "\n",
        "        vv_att = self_attention(dense_img)\n",
        "        tt_att = self_attention(dense_clinical)\n",
        "        aa_att = self_attention(dense_snp)\n",
        "\n",
        "        vt_att = cross_modal_attention(vv_att, tt_att)\n",
        "        av_att = cross_modal_attention(aa_att, vv_att)\n",
        "        ta_att = cross_modal_attention(tt_att, aa_att)\n",
        "\n",
        "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "\n",
        "    ## No Attention ##\n",
        "    elif mode == 'None':\n",
        "\n",
        "        merged = concatenate([dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "    else:\n",
        "        print (\"Mode must be one of 'MM_SA', 'MM_BA', 'MU_SA_BA' or 'None'.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    ########### Output Layer ############\n",
        "\n",
        "    output = Dense(3, activation='softmax')(merged)\n",
        "    model = Model([in_clinical, in_snp, in_img], output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train(mode, batch_size, epochs, learning_rate, seed):\n",
        "\n",
        "\n",
        "    train_clinical = pd.read_csv(\"X_train_clinical.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "    test_clinical= pd.read_csv(\"X_test_clinical.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "\n",
        "\n",
        "    train_snp = pd.read_csv(\"X_train_snp.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "    test_snp = pd.read_csv(\"X_test_snp.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "\n",
        "\n",
        "    train_img= make_img(\"X_train_img.pkl\")\n",
        "    test_img= make_img(\"X_test_img.pkl\")\n",
        "\n",
        "\n",
        "    train_label= pd.read_csv(\"y_train.csv\").drop(\"Unnamed: 0\", axis=1).values.astype(\"int\").flatten()\n",
        "    test_label= pd.read_csv(\"y_test.csv\").drop(\"Unnamed: 0\", axis=1).values.astype(\"int\").flatten()\n",
        "\n",
        "    reset_random_seeds(seed)\n",
        "    class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(train_label),y = train_label)\n",
        "    d_class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "    # compile model #\n",
        "    model = multi_modal_model(mode, train_clinical, train_snp, train_img)\n",
        "    model.compile(optimizer=Adam(learning_rate = learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "\n",
        "    # summarize results\n",
        "    history = model.fit([train_clinical,\n",
        "                         train_snp,\n",
        "                         train_img],\n",
        "                        train_label,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        class_weight=d_class_weights,\n",
        "                        validation_split=0.1,\n",
        "                        verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "    score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
        "\n",
        "    acc = score[1]\n",
        "    test_predictions = model.predict([test_clinical, test_snp, test_img])\n",
        "    cr, precision_d, recall_d, thres = calc_confusion_matrix(test_predictions, test_label, mode, learning_rate, batch_size, epochs)\n",
        "\n",
        "\n",
        "\n",
        "    plt.clf()\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.savefig('accuracy_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'.png')\n",
        "    plt.clf()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.savefig('loss_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'.png')\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # release gpu memory #\n",
        "    K.clear_session()\n",
        "    del model, history\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    print ('Mode: ', mode)\n",
        "    print ('Batch size:  ', batch_size)\n",
        "    print ('Learning rate: ', learning_rate)\n",
        "    print ('Epochs:  ', epochs)\n",
        "    print ('Test Accuracy:', '{0:.4f}'.format(acc))\n",
        "    print ('-'*55)\n",
        "\n",
        "    return acc, batch_size, learning_rate, epochs, seed\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    m_a = {}\n",
        "    seeds = random.sample(range(1, 200), 5)\n",
        "    for s in seeds:\n",
        "        acc, bs_, lr_, e_ , seed= train('MM_SA_BA', 32, 50, 0.001, s)\n",
        "        m_a[acc] = ('MM_SA_BA', acc, bs_, lr_, e_, seed)\n",
        "    print(m_a)\n",
        "    print ('-'*55)\n",
        "    max_acc = max(m_a, key=float)\n",
        "    print(\"Highest accuracy of: \" + str(max_acc) + \" with parameters: \" + str(m_a[max_acc]))\n"
      ],
      "metadata": {
        "id": "43ksPEEvXhWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "The models and evaluation metrics were all carefully implemented as explained in the original paper. However, the exact datasets that needed to be downloaded to properly execute the study were nearly impossible to figure out. There were over 1000 files to manually screen through (all with naming convention different from the original study). Furthermore, they were not from the time range that the study was conducted (i.e. I only had access to 2024 data). As such, the hypotheses I layed out where difficult to verify."
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The paper is reproducible, though acquiring the data is a struggle unless you continously follow up with the DADNI.\n",
        "\n",
        "What Is Easy: implementing the model.\n",
        "\n",
        "What Is Difficult: Acquiring the data.Ensuring accurate pre-processing of three types of data (clinical, imaging, genetics)\n",
        "\n",
        "Suggestion: Please provide pre-processed data in the materials section of the paper.\n",
        "\n",
        "Next phase: I would refine the multimodal modal and optimize hyperparameters so its more accurate in detecting Alzherimers.\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Golovanevsky, Michal, Carsten Eickhoff, and Ritambhara Singh. \"Multimodal attention-based deep learning for Alzheimer’s disease diagnosis.\" Journal of the American Medical Informatics Association 29, no. 12 (2022): 2014–2022.\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}